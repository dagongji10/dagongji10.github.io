<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬虫实例--Python百科词条]]></title>
    <url>%2F2018%2F02%2F28%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Python最基础的应用就是爬虫。本文是看了慕课网一个视频教程后根据该教程写的一个爬虫实例。这个教程讲的还是挺好的，建议刚开始学Python的童鞋可以看看。 目标：抓取百度百科Python词条相关词条网页——标题和简介入口：https://baike.baidu.com/item/PythonURL格式: --词条页面URL：/item/... 数据格式： --标题：&lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt; --简介：&lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; 页面编码：utf-8开发环境： 1、安装JRE环境：下载jdk-8u161-windows-x64，安装即可 2、安装Eclipse软件：从蒲公英上下载eclipse-java-oxygen-R-win32-x86_64，直接打开exe就是 3、安装Python 2.X：在官网下载Python 2.7.14，默认安装到C盘，之后配置环境变量---将“C:\Python27”、“C:\Python27\Scripts”添加到path路径，然后将python.exe改成python2.exe 4、环境配置：打开Eclipse》Windows》Preferences》Interpreters》Python Interpreter，在右侧新建，在弹出来的窗口第二个对话框选择python2.exe，一直确定就好。 在Eclipse中新建pyDev Project，命名imooc；右键imooc，新建pyDev package，命名baike_spider；右键baike_spider，新建pyDev module，分别命名spider_main、url_manager、html_downloader、html_parser、html_outputer。 1、调度程序:spider_main # coding:utf-8 from baike_spider import url_manager,html_downloader,html_parser,html_outputer class SpiderMain(object): def __init__(self): //初始化方法 self.urls = url_manager.UrlManager() //url管理器 self.downloader = html_downloader.HtmlDownloader() //网页下载器 self.parser = html_parser.HtmlParser() //解析器 self.outputer = html_outputer.HtmlOutputer() //内容输出器 def craw(self, root_url): count = 1 //计数器 self.urls.add_new_url(root_url) //添加根网址到url管理器 while self.urls.has_new_url(): //开始循环 try: new_url = self.urls.get_new_url() //取第一个url print &apos;craw %d : %s&apos; % (count,new_url) //输出url html_cont = self.downloader.download(new_url) //下载网页内容 new_urls,new_data = self.parser.parse(new_url,html_cont) //获取新的url和价值数据 self.urls.add_new_urls(new_urls) //添加新的url到url管理器 self.outputer.collect_data(new_data) //收集价值数据 if count == 1000: break count = count + 1 except: print &apos;craw failed&apos; self.outputer.output_html() //输出数据到网页 print &apos;finished!&apos; if __name__==&quot;__main__&quot;: root_url = &quot;https://baike.baidu.com/item/Python&quot; //原始根网址 obj_spider = SpiderMain() //主程序入口 obj_spider.craw(root_url) //爬虫启动 2、url管理器：url_manager # coding:utf-8 class UrlManager(object): def __init__(self): //初始化方法 self.new_urls = set() //未爬URL容器 self.old_urls = set() //已爬URL容器 def add_new_url(self,url): //单个URL存储方法 if url is None: return if url not in self.new_urls and url not in self.old_urls: self.new_urls.add(url) def add_new_urls(self,urls): //多个URL存储方法 if urls is None or len(urls) == 0: return for url in urls: self.new_urls.add(url) def has_new_url(self): //判断是否还有未爬URL return len(self.new_urls) != 0 def get_new_url(self): //获取一个未爬URL new_url = self.new_urls.pop() self.old_urls.add(new_url) return new_url 3、网页下载：html_downloader # coding:utf-8 import urllib2 class HtmlDownloader(object): def download(self,url): //下载网页方法 if url is None: return None response = urllib2.urlopen(url) //请求数据 if response.getcode() != 200: //状态码如果等于200，说明请求成功 return None return response.read() //读取数据 4、解释器：html_parser # coding:utf-8 from bs4 import BeautifulSoup import urlparse import re class HtmlParser(object): def parse(self,page_url,html_cont): //解释器方法：page_url是网址参数，html_cont是网页内容参数 if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont,&apos;html.parser&apos;,from_encoding=&apos;utf-8&apos;) //解释器对象 new_urls = self._get_new_urls(page_url,soup) //获取新网址，实现方法在下面 new_data = self._get_new_data(page_url,soup) //获取数据，实现方法在下面 return new_urls,new_data def _get_new_urls(self, page_url, soup): //新网址获取方法，这个要根据具体网址信息来设计，不是一成不变的 new_urls = set() //网址容器 links = soup.find_all(&apos;a&apos;,href=re.compile(r&quot;/item/&quot;)) //获取标签为a、包含“/item/”的链接信息（这个信息事实上是残缺的） for link in links: //这里用到正则表达式 new_url = link[&apos;href&apos;] new_full_url = urlparse.urljoin(page_url,new_url) //我们这里将残缺的网址信息和网址参数合并作为新的URL new_urls.add(new_full_url) return new_urls def _get_new_data(self, page_url, soup): //价值信息获取方法 res_data = {} //容器，字典结构 res_data[&apos;url&apos;] = page_url //将网址存入数据容器 # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt; &lt;h1&gt;Python&lt;/h1&gt; //这是我当时查看的网页源代码的格式 title_node = soup.find(&apos;dd&apos;,class_=&quot;lemmaWgt-lemmaTitle-title&quot;).find(&quot;h1&quot;) //获取标题节点 res_data[&apos;title&apos;] = title_node.get_text() //存储标题 # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; summary_node = soup.find(&apos;div&apos;,class_=&quot;lemma-summary&quot;) //获取简介内容节点 res_data[&apos;summary&apos;] = summary_node.get_text() //存储简介信息 return res_data 5、输出：html_outputer # coding:utf-8 class HtmlOutputer(object): def __init__(self): //初始化方法 self.datas = [] def collect_data(self,data): //数据收集方法 if data is None: return self.datas.append(data) def output_html(self): fout = open(&apos;output.html&apos;,&apos;w&apos;) //新建一个.html文件 fout.write(&quot;&lt;html&gt;&quot;) //将爬到的内容输出为html fout.write(&quot;&lt;body&gt;&quot;) //所以这里输出为html表格形式 fout.write(&quot;&lt;table&gt;&quot;) for data in self.datas: fout.write(&quot;&lt;tr&gt;&quot;) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;]) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;title&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;summary&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;/tr&gt;&quot;) fout.write(&quot;&lt;/table&gt;&quot;) fout.write(&quot;&lt;/body&gt;&quot;) fout.write(&quot;&lt;/html&gt;&quot;) fout.close() 运行spider_main，刷新baike_spider项目，会看到新生成的output.html；找到该本地文件，直接用浏览器打开，就能看到爬虫抓取的数据信息.]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+hexo+next搭建博客]]></title>
    <url>%2F2018%2F01%2F25%2FGitHub%2Bhexo%2Bnext%2F</url>
    <content type="text"><![CDATA[很多人认为新浪、CSDN的博客已经够方便了，为什么还要大费周折自己搭建呢？这两者确实给我们提供了一个方便的途径分享学习经验，但是其框架都是确定的，作者除了选择主题风格（大多数排版布局等）可选外没有太多发挥空间。本文将介绍如何利用GitHub这个平台搭建属于自己的博客，对于自己搭建的博客你可以任意发挥，从内容到格式完全的DIY，而且没有广告、本地维护。用到三个平台： Hexo是本地框架，其实还应该加上Markdown，用来写文章； GitHub主要是提供博客发布中转，就是免费给你一个域名，本地的文章提交到GitHub后他人可以通过网络访问； Next是一个比较好的主题，就好像你的手机换主题一样，根据你的喜好给博客换不同的着装。 在使用Hexo之前要配置好本地环境，这是本文的第一步；然后安装Hexo，成功以后就可以在本地访问你的第一个原始博客了；然后需要将本地博客发到GitHub上去，这是本文的第三步；最后是Next的使用，有些没介绍是因为给的链接介绍的很详细不再赘述。 1、搭建环境准备 * node.js的安装（简单） * git的安装（简单） * gitHub账户的配置（最终要得到一个GitHub的可访问网址，就是一个仓库repo） 对于上面这三个步骤网上有很多教程，对照教程就可以。 2、安装Hexo * 新建文件夹Hexo》blog；打开cmd面板，定位到空文件夹Hexo》blog * 输入：npm install hexo -g //-g表示全局安装，npm默认为当前项目安装，这个安装是默认装到C盘指定位置 * 输入：hexo init //初始化，其实就是把安装文件复制到blog里面 * 输入：hexo generate //自动根据当前目录下文件生成静态网页 * 输入：npm install hexo-server --save //安装hexo-server插件 * 输入：hexo server //启动hexo * 在网页端地址栏输入：http://localhost:4000/，就可以看到新建的hello world博客。 3、将Hexo与github page 联系起来 （1）配置git个人信息` * 在桌面鼠标右键，点击Git Bash Here * 输入：git config --global user.name &quot;GitHub的注册用户名&quot; * 输入：git config --global user.email &quot;GitHub注册邮箱&quot; * 输入：ssh-keygen -t rsa -C &quot;GitHub注册邮箱&quot;，设置文件放的位置，保存生成的密匙（id_rsa，id_rsa.pub） * 然后进入GitHub账户，点击右上方头像旁边的向下三角箭头，选择Settings，进去后选择SSH and GPG keys，新建SSH key；自己给key起一个名字，然后将上一步生成的id_rsa.pub中的内容拷贝到key对话框，保存。 * 测试》仍然在刚才的GitBash中输入：ssh -T git@github.com，如果有提示输入（yes/no?）就输入y，然后回车； * 出现“Hi （GitHub用户名）....”等字样就说明配置成功了。 （2）配置deployment * 进入GitHub，点击第1步里面生成的那个仓库，找到右方中部“Clone or Doenload”，点击后会出现一个选择下拉框，把他变成“Clone with SSH”,然后复制下面地址栏的内容（git@github.com:dagongji10/dagongji10.github.io.git） * 找到Hexo》blog里面的配置文件_config.yml，打开，修改以下内容： deploy: type: git repo: git@github.com:dagongji10/dagongji10.github.io.git //这就是本地文件和GitHub建立联系的地方 branch: master //千万要注意冒号后面有一个空格，一定不能省略 * 保存之后回到Hexo》blog的GitBash，输入：hexo generate * 输入：hexo deploy（部署博客，需要等一会儿），看到Done：git的提示就好啦 （3）在浏览器地址栏输入https://（GitHub用户名）.github.io/就可以访问了 4、写博客与博客发布 * hexo new &quot;名称&quot; //新建博客 5、布置自己的blog（参考） （1）在右上角或左上角实现fork me on GitHub： * 进入网址：https://github.com/blog/273-github-ribbons，选择喜欢的风格，复制代码； * 打开文件：themes/next/layout/_layout.swig，把刚才复制的代码放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面，并把href改为你的github地址 （2）添加RSS * 进入站点配置文件，找到Plugins，去掉前面的#，在后面添加hexo-generate-feed（plugins: hexo-generate-feed） * 进入主题配置文件，找到rss，在后面添加/atom.xml（rss: /atom.xml） （3）修改文章底部带“#”的标签 * 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成 &lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; （4）在文章末尾添加“本文结束”标记 （5）博文压缩 （6）在网站底部加上访客数量 （8）在线搜索：next官网》第三方服务有相关教程 （9）评论（来必力） * 来必力官网注册，点击网站右上角找到管理页面，在弹出的页面添加自己的网站地址，名字，之后复制代码里面data-uid后面的内容（带引号）； * 进入主题配置文件，修改livere_uid后面的值为上一步复制的代码，保存即可。 （10）在文章底部增加版权信息]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
