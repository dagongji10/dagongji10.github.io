<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorFlow+MTCNN模型训练到实例应用]]></title>
    <url>%2F2018%2F10%2F28%2FTensorFlow-MTCNN%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%88%B0%E5%AE%9E%E4%BE%8B%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[模型理解MTCNN（Tutil-Task CNN）：这里的多任务指的是人脸检测、人脸对齐、特征点定位。模型采用级联结构，由三个不同的层组成： Proposal-Net：生成候选框Refine-Net：优化候选框，去掉重叠率较小的Output-Net：继续优化，并定位特征点 实际上，每一步都会有三个输出：人脸判断，边框预测，特征点定位；只是在不同的阶段侧重点不同，前两个阶段侧重于判断人脸以及筛选边框，最后的O-Net则更关注特征点。 P-Net环节 （1）会随机生成一些边界框，其位置、尺寸存在一定的随机性，但仍然会紧紧依赖于所给的图片；（2）输入是图片，输出是人脸判断、边框检测、特征点定位，目标函数是这三者的预测值与真实值的差距和，只是此阶段人脸判断的误差占比会最大；（3）图片分为三类：negative，positive，part-face，此环节的主要任务是对这三类图片分类，并在positive类上产生脸部的边界框；（4）边界框可能会有很多是重叠的、不准确的，这时就要用回归的方式不断调整参数，回归检测的标准是输入的标签，里面有准确的脸部边框数据；（5）激活函数为pReLU，优化算法为动量梯度下降，在进行一定的学习次数之后，模型可以比较准确的进行人脸分类，并预测脸框的位置。 R-Net环节 （1）会以P-Net环节的优化模型作为基础，再次生成边界框（这时已经可以较准确的判断人脸了）；（2）同样是利用卷积神经网络来进行学习，只是卷积的模型会稍有区别（输入变为24*24，卷积结构改变）；（3）虽然这一层是优化层，主要用来去掉大量无效的边界框，但是三类输出的损失占比同P-Net层是一样的，相当于只是加深了网络而已。 O-Net环节 （1）主要是进行人脸特征点的定位学习，这里三类输出的损失占比和前两个稍有不同，特征点的损失和人脸分类相同，而弱化边界框（2）所以在这里，应该是默认人脸分类、边界框定位已经准确了，最后在边界框内部修正特征点的位置（3）实际学习中，特征点的位置用的并不是绝对坐标，而是在边界框中的相对位置，坐标范围是[0，1] 网络的重点在于：使用了边界框回归，非极大值抑制NMS；边界框回归是用来不断调整边界框的位置，NMS是用来摒弃那些不合格的边界框（刚开始产生的框是非常多的）。网络的模型并不难懂，关键在于在python+TensorFlow环境下，如何处理和操作数据（这里的数据量是非常大的，比cifar100多） 数据的处理与模型训练（缺一张文件夹列表图！）从runAll.sh文件内容可以看到，三层网络是级联关系，分阶段学习；三阶段中有一部分操作是相同的，调用的同一脚本（仅限于我所看到的框架源码） gen_hard_bbox_pnet.py主要是为pnet层产生所需的数据，说白了就是边界框，分成三类：negative，positive，part-face，用这些边界框去截取原图像，得到框选出的区域（可能是人脸，可能不是） 里面会涉及到框的随机位置产生、随机尺寸，还有可能相对于真实人脸框的偏移、缩放等；要的就是增加训练数据的复杂性，使得模型鲁棒性更强。数据部分是随机生成的，它相对于真实人脸的位置不确定，大部分是错误的人脸区域；还有一部分是从真实人脸的附近产生，接近正解（其实这一部分要不要都无所谓）。从这里可以看到，训练框的产生依赖于真实框，真实框是事先定义好的，需要从文件中读取进来；然后通过对训练框的学习来优化模型参数 gen_landmark_aug.py产生landmark数据，这个数据也是从真实数据中做一定的偏移得到的，只不过不是直接针对landmark来偏移，而是建立landmark在图片中的相对位置，旋转图片，然后对应的相对位置也发生改变 gen_tfrecords.py本地数据创建完成，包括：image，box，landmark，这里的一张图片可能包含多个box和landmark，但是box和landmark是成对出现的该脚本读取本地数据，然后存入tfrecord中，以后每次训练需要相关的数据就从tfrecord中读取，这样做的好处是速度快、占用内存小tfrecord是TensorFlow中一种管理数据的的方式，首先是要生成一个tfrecord文件，然后使用的时候读取这个文件，从里面恢复数据；tfrecord文件的格式是固定的 train.py训练脚本，同cifar100类似，里面定义了多线程管理机制，从tfrecord读取数据，利用feed_dict传入模型，得到预测值之后计算损失函数值；定义步数，学习率，优化算法，等等，然后创建一个session来运行计算图，保存计算结果（包括可视化数据） gen_hard_bbox_rnet_onet.py这是R-Net和O-Net阶段产生边界框的文件，与gen_hard_bbox_pnet.py地位相同，但是具体实现略有差别因为在P-Net阶段已经产生了tfrecord文件，那就没必要再从原始文件中去读取数据做操作了，直接在P-Net所产生的tfrecord数据基础上做操作这一步的操作依然是产生边界框，所以还是需要偏移、旋转、缩放，这是最为主要的操作，完了之后将数据重新写入到tfrecord文件虽然改变了tfrecord文件，但是之前的数据用过了，后面的阶段自己会产生数据，所以也就无所谓 commom_utils.py定义了一些常用的方法，比如计算iou，方格化图片，等 tfrecord_utils.py，tfrecord_reader.py定义了一些tfrecord文件的操作方法 landmark_utils.py定义了画边界框、特征点的方法 mtcnn_model.py定义了三个模型的结构，如何形成卷积神经网络的，卷积个数、大小、池化等等 总结训练的时候，可以修改runAll.sh文件，分别只用到pnet、rnet、onet三部分，分别执行三次，就可以完成模型的训练。模型训练完成之后，结果会保存在tmp文件夹中，model中就是三个阶段的模型 实例测试测试代码在testing文件夹中，运行”python test_images.py –stage=pnet”可以测试pnet，rnet/onet类似 test_images.py：获取模型的路径、待测试的图片路径，利用mtcnndector来计算人脸位置，并输出；调用了MtcnnDetector.py脚本 MtcnnDetector.py：detect_face是入口，会调用pnet、rnet、onet的检测方法，然后不同的阶段分别进行预测，预测时用到detector方法；调用detector.py脚本 detector.py：这是真正做预测的地方，首先根据test_images.py保存的模型路径来读取模型数据，然后将数据传入模型，进行预测，返回数据 输入文件：存放在images文件夹下，几张图片即可输出文件：存放在resoults_xxx文件夹下，是检测到的人脸图像 下图是三个阶段的评判阈值，修改这里的参数，结果会有差别；第一个参数用在pnet层，大于这个值被认为合格，这个值太大可能导致后面rnet、onet检测不到结果；后面同理。问题：（1）onet相对于rnet、rnet相对于pnet，框的数量在减少，这个减少的操作在哪里完成？应该是有一个判断舍弃的？答：通过py_nms函数完成的，这个函数将框与框之间的重叠区域界定到一个范围；实际上如果大部分都是重叠的，说明两个框表示同一张人脸，如果两个框重叠区域较小，那肯定要去掉一个，保存最优的；如果没有重叠区域，要么是错误，要么是属于两个人脸。]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>MTCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe+GoogLeNet模型训练]]></title>
    <url>%2F2018%2F10%2F28%2FCaffe-GoogLeNet%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[数据与配置文件上图是Caffe框架下的GoogLeNet模型文件。Caffe框架跟TensorFlow稍有区别，它的网络结构不是在脚本中，而是通过一个后缀为.prototxt的配置文件来存储，包括模型用到的一些超参数。这里用到的模型其实已经训练完成了，模型数据存储在一个后缀名为.caffemodel的二进制文件中。上图的文件描述并不完全准确，在学习该模型时也没有去深究各文件的内容，而是着重关注Caffe框架、GoogLeNet模型的结构。至于代码运行过程中的细节流程、函数调用方式等，仍需要后续补充。 Caffe框架Caffe框架主要有四个部分组成： （1）Blob：可以理解为数据流，包括训练数据、参数数据等，它支持在CPU和GPU上存储，能够在两者之间做同步（2）layer：网络的层结构，例如卷积层、池化层、激活层等，每种层都实现了前向和后向传播，层之间的数据通过Blob的形式来传递（3）net：就是网络模型的整体表示，其实就是我们的神经网络模型（4）solver：对Net模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络训练过程以上最重要的应该是layer和solver两部分。 layer分为很多不同的类型，例如：输入层，视觉层，loss层，等等。每一层都有一种自己的定义方式，里面包含一些不同的参数、参数值，需要在设计网络结构时分别写入配置文件。solver层是最重要的，它定义了如何来优化参数，相当于TensorFlow中的优化算法的选择，caffe定义了六种solver方法：SGD，AdaDelta，AdaGrad，Adam，Nesterov，RMSProp。 caffe中的三种参数： （1）可学习参数：就是需要反复训练不断优化的参数，例如：权重，偏置，等（2）结构参数：用来构成网络模型的结构参数，例如：卷积层数目，卷积核大小，卷积核数量，等等（3）训练超参数：控制训练收敛的一系列参数，可以手动设置，也可以让它自动更改，例如：迭代次数 GoogLeNet模型结构阅读配置文件中的deploy.prototxt文件，可以得到GoogLeNet模型的结构，但是这样太麻烦，可以直接阅读文献，也可以直接参考网上的博客。可以发现，GoogLeNet模型其实发展来源于LeNet-5模型，只是它把LeNet-5做的更深更宽了；这样做的好处是，分类、检测的结果会更好，因为网络越深学习越深入、网络越宽提取特征越完整细致。但是，更深更宽带来的影响是：计算复杂度加大，且容易引起梯度爆炸或者梯度消失。解决这个问题的办法是：引入inception节点，使用Hebbian原理、多尺度处理来进行优化。 GoogLeNet的相关工作： （1）对inception中的所有滤波器进行学习，并运用至22层网络》》多尺度处理（2）使用 1x1 卷积来增加网络深度，并用它来降维、限制网络尺寸（3）使用多边框预测，使用更好的网络结构 inception结构： 用密集成分来近似局部稀疏结构，这个结构让网络变得更宽，并且用到多尺度处理；在每一个尺度之前运用 1x1 卷积来进行降维 Hebbian原理： 一个很通俗的现象，先摇铃铛，之后给一只狗喂食，久而久之，狗听到铃铛就会口水连连，这也就是狗的“听到”铃铛的神经元与“控制”流口水的神经元之间的链接被加强了；而Hebbian原理的精确表达就是如果两个神经元常常同时产生动作电位，或者说同时激动，这两个神经元之间的连接就会变强，反之则变弱。 辅助loss的引入： 22层网络会让梯度消失，所以在不同的位置设置两个辅助loss，它们对最终的结果影响比例占0.3 GoogLeNet结构图： 代码解析（待补充！）]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>GoogLeNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow+cifar10模型训练到实例应用]]></title>
    <url>%2F2018%2F10%2F21%2FTensorFlow-cifar10%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%88%B0%E5%AE%9E%E4%BE%8B%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据集 概述该数据集包括60000张RGB图像，每张图像尺寸为32*32，分为10个类，每类6000张。10个类分别是：飞机，汽车，鸟，猫，鹿，狗，青蛙，马，船，卡车。60000张图像中，50000张用于训练，10000张用于测试；测试数据分为5批，每批10000张，其中每类图像数量不一定相等。 Python版本数据在这里下载，MATLAB版本数据在这里下载，二进制版本数据在这里下载。 数据布局60000张图像分为6批。数据通过cPickle库处理，产生序列化对象，在实际编码中可以通过Python语言读取这些对象，得到一个字典结构的数据；如此一来，每个batch就都包含一个字典，字典里面包含两个元素：data，labels。 data //一个10000*3072的numpy数组，数据类型为无符号整型 //10000表示图像数量，每一行就是一张图像 //3072表示的是一张独立图像的数据，每张图像尺寸为32*32，RGB三通道（32*32*3） //图像按照行顺序存储，就是说原始图像R通道像素矩阵的第一行32个值就存储在该数组某一行的前32位 labels //一维数组，包含10000个元素，每个元素取值范围0~9，代表图像所属类别 以上是Python/MATLAB版本的数据，还有一种是二进制，也就是TensorFlow所采用的。同样将数据分成6个batch。除此之外还有一个batches.meta.txt文件，这是一个ASCII文件，同样是把0~9类数字类标同每一类类名对应起来。 &lt;1 x label&gt;&lt;3072 x pixel&gt; ... &lt;1 x label&gt;&lt;3072 x pixel&gt; //第一个字节是类别，在0~9之间 //接下来是3072个字节，内容就是一幅图像的数据（同Python版本类似） //每个文件都是10000行这样的数据，没有分隔行，所以文件大小就是30730000字节 //按行，height*weight*deepth的方式，前1024字节为R通道，然后是G通道，然后是B通道 代码解析 下图是CIFAR10文件内容介绍： cifar10_input.py该文件定义了模型如何数据文件（cifar10_data中的文件），在读取完成后还要对图像进行随机剪裁，翻转，标准化，等等。经过这一系列的预处理之后，数据才可以作为模型的标准输入。 cifar10.py接下来是模型定义文件，该文件内容包括：数据输入方法，模型的建立，训练方法等。下面这张图在文件的头部，相当于是文件内容的一个总述。 它给出了整个模型的大致框架： * 首先通过distorted_inputs()方法读取数据，并存入变量inputs（图像数据）和labels（分类标签）中 * 然后通过inference()方法来完成预测，其实就是神经网络模型的搭建过程，包括：卷积，池化，局部响应归一化，全连接，softmax归一化等；预测结果存入变量predictions * 最后是训练，主要是计算损失loss、计算梯度、进行变量更新以及呈现最终结果等； 上表给出了该脚本定义的所有方法功能，其中有三个比较重要的函数方法：inference()，loss()，train()。根据inference()可以得到CNN的模型如下： cifar10_train.py模型的训练方法。 cifar10_eval.py测试数据时用到的脚本。里面定义了评估模型的方法，还有评估时的输出内容。后面如果需要实现自定义一张图片作为输入让模型来进行预测，主要是需要修改该文件中的代码。 代码运行 首先需要用训练数据作为输入，让模型不断学习，优化模型参数。训练完成后，模型确定，可以用测试数据来测试模型的准确性。依次执行如下命令： python cifar10_train.py python cifar10_eval.py 实例——单张图片作为输入 在成功运行脚本的基础上，我们想让模型对我们自定义的输入图片进行预测，然后输出该图片属于哪一类。这就需要修改脚本，主要的修改对象是cifar10_eval.py。 1、定义一个读取操作源码中的读取是基于.bin文件的，这是事先处理过的，但实际上输入模型的数据并不是.bin文件，而是一个结构为【128，24，24，3】的张量，所以我们需要把待预测的单张图片处理成模型所需要的结构形式，同时要对图像进行标准化等处理（CIFAR就这么干了，模型的需要，不处理的话结果不正确）。这相当于是一个输入接口，运行时输入python-cifar10_eval()，程序会提示输入文件名称，注意在输入文件名时要加入单引号（例如’cat.jpg’）。 2、修改cifar-10eval.py中的evaluate()方法这个方法就是模型数据的入口，源码中有两个输入：images，labels，但实际上我们只需要image，因为我们不希望去和label对比，而是直接输出检测结果。这里iamge值获取就用到上面的read_image()方法。 3、修改cifar-10eval.py中的eval_once()方法这里是程序的输出位置，源码会输出测试后的准确率，这个准确率是通过模型预测值和labels正确值之间进行比对来确定的。模型预测完后，得到logits，这是一个结构为【128，10】的张量，每一行代表一张图片的结果，行向量中最大值所在位置就代表这张图属于哪一个类。 4、结果并不是每张图片都能准确预测，有的会出错；在我的实例中，cat，ship，horse，deer，frog，airplane，一般是可以较准确分类；bird预测为airplane，automobile预测为ship：可能是在图片的中央，二者平面拓扑结构比较相似；truck预测为airplane，dog预测为airplane。 上图输出了模型预测的输入图片最有可能的类：cat，dog，bird。]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>CIFAR10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫实例--Python百科词条]]></title>
    <url>%2F2018%2F02%2F28%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Python最基础的应用就是爬虫。本文是看了慕课网一个视频教程后根据该教程写的一个爬虫实例。这个教程讲的还是挺好的，建议刚开始学Python的童鞋可以看看。 目标：抓取百度百科Python词条相关词条网页——标题和简介入口：https://baike.baidu.com/item/PythonURL格式: --词条页面URL：/item/... 数据格式： --标题：&lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt; --简介：&lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; 页面编码：utf-8开发环境： 1、安装JRE环境：下载jdk-8u161-windows-x64，安装即可 2、安装Eclipse软件：从蒲公英上下载eclipse-java-oxygen-R-win32-x86_64，直接打开exe就是 3、安装Python 2.X：在官网下载Python 2.7.14，默认安装到C盘，之后配置环境变量---将“C:\Python27”、“C:\Python27\Scripts”添加到path路径，然后将python.exe改成python2.exe 4、环境配置：打开Eclipse》Windows》Preferences》Interpreters》Python Interpreter，在右侧新建，在弹出来的窗口第二个对话框选择python2.exe，一直确定就好。 在Eclipse中新建pyDev Project，命名imooc；右键imooc，新建pyDev package，命名baike_spider；右键baike_spider，新建pyDev module，分别命名spider_main、url_manager、html_downloader、html_parser、html_outputer。 1、调度程序:spider_main # coding:utf-8 from baike_spider import url_manager,html_downloader,html_parser,html_outputer class SpiderMain(object): def __init__(self): //初始化方法 self.urls = url_manager.UrlManager() //url管理器 self.downloader = html_downloader.HtmlDownloader() //网页下载器 self.parser = html_parser.HtmlParser() //解析器 self.outputer = html_outputer.HtmlOutputer() //内容输出器 def craw(self, root_url): count = 1 //计数器 self.urls.add_new_url(root_url) //添加根网址到url管理器 while self.urls.has_new_url(): //开始循环 try: new_url = self.urls.get_new_url() //取第一个url print &apos;craw %d : %s&apos; % (count,new_url) //输出url html_cont = self.downloader.download(new_url) //下载网页内容 new_urls,new_data = self.parser.parse(new_url,html_cont) //获取新的url和价值数据 self.urls.add_new_urls(new_urls) //添加新的url到url管理器 self.outputer.collect_data(new_data) //收集价值数据 if count == 1000: break count = count + 1 except: print &apos;craw failed&apos; self.outputer.output_html() //输出数据到网页 print &apos;finished!&apos; if __name__==&quot;__main__&quot;: root_url = &quot;https://baike.baidu.com/item/Python&quot; //原始根网址 obj_spider = SpiderMain() //主程序入口 obj_spider.craw(root_url) //爬虫启动 2、url管理器：url_manager # coding:utf-8 class UrlManager(object): def __init__(self): //初始化方法 self.new_urls = set() //未爬URL容器 self.old_urls = set() //已爬URL容器 def add_new_url(self,url): //单个URL存储方法 if url is None: return if url not in self.new_urls and url not in self.old_urls: self.new_urls.add(url) def add_new_urls(self,urls): //多个URL存储方法 if urls is None or len(urls) == 0: return for url in urls: self.new_urls.add(url) def has_new_url(self): //判断是否还有未爬URL return len(self.new_urls) != 0 def get_new_url(self): //获取一个未爬URL new_url = self.new_urls.pop() self.old_urls.add(new_url) return new_url 3、网页下载：html_downloader # coding:utf-8 import urllib2 class HtmlDownloader(object): def download(self,url): //下载网页方法 if url is None: return None response = urllib2.urlopen(url) //请求数据 if response.getcode() != 200: //状态码如果等于200，说明请求成功 return None return response.read() //读取数据 4、解释器：html_parser # coding:utf-8 from bs4 import BeautifulSoup import urlparse import re class HtmlParser(object): def parse(self,page_url,html_cont): //解释器方法：page_url是网址参数，html_cont是网页内容参数 if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont,&apos;html.parser&apos;,from_encoding=&apos;utf-8&apos;) //解释器对象 new_urls = self._get_new_urls(page_url,soup) //获取新网址，实现方法在下面 new_data = self._get_new_data(page_url,soup) //获取数据，实现方法在下面 return new_urls,new_data def _get_new_urls(self, page_url, soup): //新网址获取方法，这个要根据具体网址信息来设计，不是一成不变的 new_urls = set() //网址容器 links = soup.find_all(&apos;a&apos;,href=re.compile(r&quot;/item/&quot;)) //获取标签为a、包含“/item/”的链接信息（这个信息事实上是残缺的） for link in links: //这里用到正则表达式 new_url = link[&apos;href&apos;] new_full_url = urlparse.urljoin(page_url,new_url) //我们这里将残缺的网址信息和网址参数合并作为新的URL new_urls.add(new_full_url) return new_urls def _get_new_data(self, page_url, soup): //价值信息获取方法 res_data = {} //容器，字典结构 res_data[&apos;url&apos;] = page_url //将网址存入数据容器 # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt; &lt;h1&gt;Python&lt;/h1&gt; //这是我当时查看的网页源代码的格式 title_node = soup.find(&apos;dd&apos;,class_=&quot;lemmaWgt-lemmaTitle-title&quot;).find(&quot;h1&quot;) //获取标题节点 res_data[&apos;title&apos;] = title_node.get_text() //存储标题 # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; summary_node = soup.find(&apos;div&apos;,class_=&quot;lemma-summary&quot;) //获取简介内容节点 res_data[&apos;summary&apos;] = summary_node.get_text() //存储简介信息 return res_data 5、输出：html_outputer # coding:utf-8 class HtmlOutputer(object): def __init__(self): //初始化方法 self.datas = [] def collect_data(self,data): //数据收集方法 if data is None: return self.datas.append(data) def output_html(self): fout = open(&apos;output.html&apos;,&apos;w&apos;) //新建一个.html文件 fout.write(&quot;&lt;html&gt;&quot;) //将爬到的内容输出为html fout.write(&quot;&lt;body&gt;&quot;) //所以这里输出为html表格形式 fout.write(&quot;&lt;table&gt;&quot;) for data in self.datas: fout.write(&quot;&lt;tr&gt;&quot;) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;]) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;title&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;summary&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;/tr&gt;&quot;) fout.write(&quot;&lt;/table&gt;&quot;) fout.write(&quot;&lt;/body&gt;&quot;) fout.write(&quot;&lt;/html&gt;&quot;) fout.close() 运行spider_main，刷新baike_spider项目，会看到新生成的output.html；找到该本地文件，直接用浏览器打开，就能看到爬虫抓取的数据信息.]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+hexo+next搭建博客]]></title>
    <url>%2F2018%2F01%2F25%2FGitHub%2Bhexo%2Bnext%2F</url>
    <content type="text"><![CDATA[GitHub搭建属于自己的博客，一方面博客主题可以随心所欲（即便不是自己开发，也可选择套用一些比较优美的基础库），另一方面没有各种广告的入侵。同时在本地维护，完全由自己掌控。主要用到三个平台： Hexo是本地框架，其实还应该加上Markdown，用来写文章； GitHub主要是提供博客发布中转，就是免费给你一个域名，本地的文章提交到GitHub后他人可以通过网络访问； Next是一个比较好的主题，就好像你的手机换主题一样，根据你的喜好给博客换不同的着装。 在使用Hexo之前要配置好本地环境，这是本文的第一步；然后安装Hexo，成功以后就可以在本地访问你的第一个原始博客了；然后需要将本地博客发到GitHub上去，这是本文的第三步；最后是Next的使用，有些没介绍是因为给的链接介绍的很详细不再赘述。 1、搭建环境准备 * node.js的安装（简单） * git的安装（简单） * gitHub账户的配置（最终要得到一个GitHub的可访问网址，就是一个仓库repo） 对于上面这三个步骤网上有很多教程，对照教程就可以。 2、安装Hexo * 新建文件夹Hexo》blog；打开cmd面板，定位到空文件夹Hexo》blog * 输入：npm install hexo -g //-g表示全局安装，npm默认为当前项目安装，这个安装是默认装到C盘指定位置 * 输入：hexo init //初始化，其实就是把安装文件复制到blog里面 * 输入：hexo generate //自动根据当前目录下文件生成静态网页 * 输入：npm install hexo-server --save //安装hexo-server插件 * 输入：hexo server //启动hexo * 在网页端地址栏输入：http://localhost:4000/，就可以看到新建的hello world博客。 3、将Hexo与github page 联系起来 （1）配置git个人信息` * 在桌面鼠标右键，点击Git Bash Here * 输入：git config --global user.name &quot;GitHub的注册用户名&quot; * 输入：git config --global user.email &quot;GitHub注册邮箱&quot; * 输入：ssh-keygen -t rsa -C &quot;GitHub注册邮箱&quot;，设置文件放的位置，保存生成的密匙（id_rsa，id_rsa.pub） * 然后进入GitHub账户，点击右上方头像旁边的向下三角箭头，选择Settings，进去后选择SSH and GPG keys，新建SSH key；自己给key起一个名字，然后将上一步生成的id_rsa.pub中的内容拷贝到key对话框，保存。 * 测试》仍然在刚才的GitBash中输入：ssh -T git@github.com，如果有提示输入（yes/no?）就输入y，然后回车； * 出现“Hi （GitHub用户名）....”等字样就说明配置成功了。 （2）配置deployment * 进入GitHub，点击第1步里面生成的那个仓库，找到右方中部“Clone or Doenload”，点击后会出现一个选择下拉框，把他变成“Clone with SSH”,然后复制下面地址栏的内容（git@github.com:dagongji10/dagongji10.github.io.git） * 找到Hexo》blog里面的配置文件_config.yml，打开，修改以下内容： deploy: type: git repo: git@github.com:dagongji10/dagongji10.github.io.git //这就是本地文件和GitHub建立联系的地方 branch: master //千万要注意冒号后面有一个空格，一定不能省略 * 保存之后回到Hexo》blog的GitBash，输入：hexo generate * 输入：hexo deploy（部署博客，需要等一会儿），看到Done：git的提示就好啦 （3）在浏览器地址栏输入https://（GitHub用户名）.github.io/就可以访问了 4、写博客与博客发布 * 进入本地博客目录，鼠标右键》Git Bash Here //进入hexo环境 * hexo clean //清除本地缓存 * hexo generate //生成个人博客所需的静态页面 * hexo new &quot;名称&quot; //新建博客 * 然后进入博客目录，找到 ..\source\_posts 路径 //这里就是博客的本地文件 * 用Markdown来修改博客 * hexo deploy //发布本地博客 5、布置自己的blog（参考） （1）在右上角或左上角实现fork me on GitHub： * 进入网址：https://github.com/blog/273-github-ribbons，选择喜欢的风格，复制代码； * 打开文件：themes/next/layout/_layout.swig，把刚才复制的代码放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面，并把href改为你的github地址 （2）添加RSS * 进入站点配置文件，找到Plugins，去掉前面的#，在后面添加hexo-generate-feed（plugins: hexo-generate-feed） * 进入主题配置文件，找到rss，在后面添加/atom.xml（rss: /atom.xml） （3）修改文章底部带“#”的标签 * 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成 &lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; （4）在文章末尾添加“本文结束”标记 （5）博文压缩 （6）在网站底部加上访客数量 （8）在线搜索：next官网》第三方服务有相关教程 （9）评论（来必力） * 来必力官网注册，点击网站右上角找到管理页面，在弹出的页面添加自己的网站地址，名字，之后复制代码里面data-uid后面的内容（带引号）； * 进入主题配置文件，修改livere_uid后面的值为上一步复制的代码，保存即可。 （10）在文章底部增加版权信息]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
