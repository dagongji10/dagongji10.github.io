<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorFlow+cifar10模型训练到实例应用]]></title>
    <url>%2F2018%2F10%2F21%2FTensorFlow-cifar10%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%88%B0%E5%AE%9E%E4%BE%8B%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据集 概述该数据集包括60000张RGB图像，每张图像尺寸为32*32，分为10个类，每类6000张。10个类分别是：飞机，汽车，鸟，猫，鹿，狗，青蛙，马，船，卡车。60000张图像中，50000张用于训练，10000张用于测试；测试数据分为5批，每批10000张，其中每类图像数量不一定相等。 Python版本数据在这里下载，MATLAB版本数据在这里下载，二进制版本数据在这里下载。 数据布局60000张图像分为6批。数据通过cPickle库处理，产生序列化对象，在实际编码中可以通过Python语言读取这些对象，得到一个字典结构的数据；如此一来，每个batch就都包含一个字典，字典里面包含两个元素：data，labels。 data //一个10000*3072的numpy数组，数据类型为无符号整型 //10000表示图像数量，每一行就是一张图像 //3072表示的是一张独立图像的数据，每张图像尺寸为32*32，RGB三通道（32*32*3） //图像按照行顺序存储，就是说原始图像R通道像素矩阵的第一行32个值就存储在该数组某一行的前32位 labels //一维数组，包含10000个元素，每个元素取值范围0~9，代表图像所属类别 以上是Python/MATLAB版本的数据，还有一种是二进制，也就是TensorFlow所采用的。同样将数据分成6个batch。除此之外还有一个batches.meta.txt文件，这是一个ASCII文件，同样是把0~9类数字类标同每一类类名对应起来。 &lt;1 x label&gt;&lt;3072 x pixel&gt; ... &lt;1 x label&gt;&lt;3072 x pixel&gt; //第一个字节是类别，在0~9之间 //接下来是3072个字节，内容就是一幅图像的数据（同Python版本类似） //每个文件都是10000行这样的数据，没有分隔行，所以文件大小就是30730000字节 //按行，height*weight*deepth的方式，前1024字节为R通道，然后是G通道，然后是B通道 代码解析 下图是CIFAR10文件内容介绍： cifar10_input.py该文件定义了模型如何数据文件（cifar10_data中的文件），在读取完成后还要对图像进行随机剪裁，翻转，标准化，等等。经过这一系列的预处理之后，数据才可以作为模型的标准输入。 cifar10.py接下来是模型定义文件，该文件内容包括：数据输入方法，模型的建立，训练方法等。下面这张图在文件的头部，相当于是文件内容的一个总述。 它给出了整个模型的大致框架： * 首先通过distorted_inputs()方法读取数据，并存入变量inputs（图像数据）和labels（分类标签）中 * 然后通过inference()方法来完成预测，其实就是神经网络模型的搭建过程，包括：卷积，池化，局部响应归一化，全连接，softmax归一化等；预测结果存入变量predictions * 最后是训练，主要是计算损失loss、计算梯度、进行变量更新以及呈现最终结果等； 上表给出了该脚本定义的所有方法功能，其中有三个比较重要的函数方法：inference()，loss()，train()。根据inference()可以得到CNN的模型如下： cifar10_train.py模型的训练方法。 cifar10_eval.py测试数据时用到的脚本。里面定义了评估模型的方法，还有评估时的输出内容。后面如果需要实现自定义一张图片作为输入让模型来进行预测，主要是需要修改该文件中的代码。 代码运行 首先需要用训练数据作为输入，让模型不断学习，优化模型参数。训练完成后，模型确定，可以用测试数据来测试模型的准确性。依次执行如下命令： python cifar10_train.py python cifar10_eval.py 实例——单张图片作为输入 在成功运行脚本的基础上，我们想让模型对我们自定义的输入图片进行预测，然后输出该图片属于哪一类。这就需要修改脚本，主要的修改对象是cifar10_eval.py。 1、定义一个读取操作源码中的读取是基于.bin文件的，这是事先处理过的，但实际上输入模型的数据并不是.bin文件，而是一个结构为【128，24，24，3】的张量，所以我们需要把待预测的单张图片处理成模型所需要的结构形式，同时要对图像进行标准化等处理（CIFAR就这么干了，模型的需要，不处理的话结果不正确）。这相当于是一个输入接口，运行时输入python-cifar10_eval()，程序会提示输入文件名称，注意在输入文件名时要加入单引号（例如’cat.jpg’）。 2、修改cifar-10eval.py中的evaluate()方法这个方法就是模型数据的入口，源码中有两个输入：images，labels，但实际上我们只需要image，因为我们不希望去和label对比，而是直接输出检测结果。这里iamge值获取就用到上面的read_image()方法。 3、修改cifar-10eval.py中的eval_once()方法这里是程序的输出位置，源码会输出测试后的准确率，这个准确率是通过模型预测值和labels正确值之间进行比对来确定的。模型预测完后，得到logits，这是一个结构为【128，10】的张量，每一行代表一张图片的结果，行向量中最大值所在位置就代表这张图属于哪一个类。 4、结果并不是每张图片都能准确预测，有的会出错；在我的实例中，cat，ship，horse，deer，frog，airplane，一般是可以较准确分类；bird预测为airplane，automobile预测为ship：可能是在图片的中央，二者平面拓扑结构比较相似；truck预测为airplane，dog预测为airplane。 上图输出了模型预测的输入图片最有可能的类：cat，dog，bird。]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>CIFAR10</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫实例--Python百科词条]]></title>
    <url>%2F2018%2F02%2F28%2F%E7%88%AC%E8%99%AB%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Python最基础的应用就是爬虫。本文是看了慕课网一个视频教程后根据该教程写的一个爬虫实例。这个教程讲的还是挺好的，建议刚开始学Python的童鞋可以看看。 目标：抓取百度百科Python词条相关词条网页——标题和简介入口：https://baike.baidu.com/item/PythonURL格式: --词条页面URL：/item/... 数据格式： --标题：&lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt;&lt;h1&gt;Python&lt;/h1&gt; --简介：&lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; 页面编码：utf-8开发环境： 1、安装JRE环境：下载jdk-8u161-windows-x64，安装即可 2、安装Eclipse软件：从蒲公英上下载eclipse-java-oxygen-R-win32-x86_64，直接打开exe就是 3、安装Python 2.X：在官网下载Python 2.7.14，默认安装到C盘，之后配置环境变量---将“C:\Python27”、“C:\Python27\Scripts”添加到path路径，然后将python.exe改成python2.exe 4、环境配置：打开Eclipse》Windows》Preferences》Interpreters》Python Interpreter，在右侧新建，在弹出来的窗口第二个对话框选择python2.exe，一直确定就好。 在Eclipse中新建pyDev Project，命名imooc；右键imooc，新建pyDev package，命名baike_spider；右键baike_spider，新建pyDev module，分别命名spider_main、url_manager、html_downloader、html_parser、html_outputer。 1、调度程序:spider_main # coding:utf-8 from baike_spider import url_manager,html_downloader,html_parser,html_outputer class SpiderMain(object): def __init__(self): //初始化方法 self.urls = url_manager.UrlManager() //url管理器 self.downloader = html_downloader.HtmlDownloader() //网页下载器 self.parser = html_parser.HtmlParser() //解析器 self.outputer = html_outputer.HtmlOutputer() //内容输出器 def craw(self, root_url): count = 1 //计数器 self.urls.add_new_url(root_url) //添加根网址到url管理器 while self.urls.has_new_url(): //开始循环 try: new_url = self.urls.get_new_url() //取第一个url print &apos;craw %d : %s&apos; % (count,new_url) //输出url html_cont = self.downloader.download(new_url) //下载网页内容 new_urls,new_data = self.parser.parse(new_url,html_cont) //获取新的url和价值数据 self.urls.add_new_urls(new_urls) //添加新的url到url管理器 self.outputer.collect_data(new_data) //收集价值数据 if count == 1000: break count = count + 1 except: print &apos;craw failed&apos; self.outputer.output_html() //输出数据到网页 print &apos;finished!&apos; if __name__==&quot;__main__&quot;: root_url = &quot;https://baike.baidu.com/item/Python&quot; //原始根网址 obj_spider = SpiderMain() //主程序入口 obj_spider.craw(root_url) //爬虫启动 2、url管理器：url_manager # coding:utf-8 class UrlManager(object): def __init__(self): //初始化方法 self.new_urls = set() //未爬URL容器 self.old_urls = set() //已爬URL容器 def add_new_url(self,url): //单个URL存储方法 if url is None: return if url not in self.new_urls and url not in self.old_urls: self.new_urls.add(url) def add_new_urls(self,urls): //多个URL存储方法 if urls is None or len(urls) == 0: return for url in urls: self.new_urls.add(url) def has_new_url(self): //判断是否还有未爬URL return len(self.new_urls) != 0 def get_new_url(self): //获取一个未爬URL new_url = self.new_urls.pop() self.old_urls.add(new_url) return new_url 3、网页下载：html_downloader # coding:utf-8 import urllib2 class HtmlDownloader(object): def download(self,url): //下载网页方法 if url is None: return None response = urllib2.urlopen(url) //请求数据 if response.getcode() != 200: //状态码如果等于200，说明请求成功 return None return response.read() //读取数据 4、解释器：html_parser # coding:utf-8 from bs4 import BeautifulSoup import urlparse import re class HtmlParser(object): def parse(self,page_url,html_cont): //解释器方法：page_url是网址参数，html_cont是网页内容参数 if page_url is None or html_cont is None: return soup = BeautifulSoup(html_cont,&apos;html.parser&apos;,from_encoding=&apos;utf-8&apos;) //解释器对象 new_urls = self._get_new_urls(page_url,soup) //获取新网址，实现方法在下面 new_data = self._get_new_data(page_url,soup) //获取数据，实现方法在下面 return new_urls,new_data def _get_new_urls(self, page_url, soup): //新网址获取方法，这个要根据具体网址信息来设计，不是一成不变的 new_urls = set() //网址容器 links = soup.find_all(&apos;a&apos;,href=re.compile(r&quot;/item/&quot;)) //获取标签为a、包含“/item/”的链接信息（这个信息事实上是残缺的） for link in links: //这里用到正则表达式 new_url = link[&apos;href&apos;] new_full_url = urlparse.urljoin(page_url,new_url) //我们这里将残缺的网址信息和网址参数合并作为新的URL new_urls.add(new_full_url) return new_urls def _get_new_data(self, page_url, soup): //价值信息获取方法 res_data = {} //容器，字典结构 res_data[&apos;url&apos;] = page_url //将网址存入数据容器 # &lt;dd class=&quot;lemmaWgt-lemmaTitle-title&quot;&gt; &lt;h1&gt;Python&lt;/h1&gt; //这是我当时查看的网页源代码的格式 title_node = soup.find(&apos;dd&apos;,class_=&quot;lemmaWgt-lemmaTitle-title&quot;).find(&quot;h1&quot;) //获取标题节点 res_data[&apos;title&apos;] = title_node.get_text() //存储标题 # &lt;div class=&quot;lemma-summary&quot; label-module=&quot;lemmaSummary&quot;&gt; summary_node = soup.find(&apos;div&apos;,class_=&quot;lemma-summary&quot;) //获取简介内容节点 res_data[&apos;summary&apos;] = summary_node.get_text() //存储简介信息 return res_data 5、输出：html_outputer # coding:utf-8 class HtmlOutputer(object): def __init__(self): //初始化方法 self.datas = [] def collect_data(self,data): //数据收集方法 if data is None: return self.datas.append(data) def output_html(self): fout = open(&apos;output.html&apos;,&apos;w&apos;) //新建一个.html文件 fout.write(&quot;&lt;html&gt;&quot;) //将爬到的内容输出为html fout.write(&quot;&lt;body&gt;&quot;) //所以这里输出为html表格形式 fout.write(&quot;&lt;table&gt;&quot;) for data in self.datas: fout.write(&quot;&lt;tr&gt;&quot;) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;url&apos;]) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;title&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;td&gt;%s&lt;/td&gt;&quot; % data[&apos;summary&apos;].encode(&apos;utf-8&apos;)) fout.write(&quot;&lt;/tr&gt;&quot;) fout.write(&quot;&lt;/table&gt;&quot;) fout.write(&quot;&lt;/body&gt;&quot;) fout.write(&quot;&lt;/html&gt;&quot;) fout.close() 运行spider_main，刷新baike_spider项目，会看到新生成的output.html；找到该本地文件，直接用浏览器打开，就能看到爬虫抓取的数据信息.]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub+hexo+next搭建博客]]></title>
    <url>%2F2018%2F01%2F25%2FGitHub%2Bhexo%2Bnext%2F</url>
    <content type="text"><![CDATA[GitHub搭建属于自己的博客，一方面博客主题可以随心所欲（即便不是自己开发，也可选择套用一些比较优美的基础库），另一方面没有各种广告的入侵。同时在本地维护，完全由自己掌控。主要用到三个平台： Hexo是本地框架，其实还应该加上Markdown，用来写文章； GitHub主要是提供博客发布中转，就是免费给你一个域名，本地的文章提交到GitHub后他人可以通过网络访问； Next是一个比较好的主题，就好像你的手机换主题一样，根据你的喜好给博客换不同的着装。 在使用Hexo之前要配置好本地环境，这是本文的第一步；然后安装Hexo，成功以后就可以在本地访问你的第一个原始博客了；然后需要将本地博客发到GitHub上去，这是本文的第三步；最后是Next的使用，有些没介绍是因为给的链接介绍的很详细不再赘述。 1、搭建环境准备 * node.js的安装（简单） * git的安装（简单） * gitHub账户的配置（最终要得到一个GitHub的可访问网址，就是一个仓库repo） 对于上面这三个步骤网上有很多教程，对照教程就可以。 2、安装Hexo * 新建文件夹Hexo》blog；打开cmd面板，定位到空文件夹Hexo》blog * 输入：npm install hexo -g //-g表示全局安装，npm默认为当前项目安装，这个安装是默认装到C盘指定位置 * 输入：hexo init //初始化，其实就是把安装文件复制到blog里面 * 输入：hexo generate //自动根据当前目录下文件生成静态网页 * 输入：npm install hexo-server --save //安装hexo-server插件 * 输入：hexo server //启动hexo * 在网页端地址栏输入：http://localhost:4000/，就可以看到新建的hello world博客。 3、将Hexo与github page 联系起来 （1）配置git个人信息` * 在桌面鼠标右键，点击Git Bash Here * 输入：git config --global user.name &quot;GitHub的注册用户名&quot; * 输入：git config --global user.email &quot;GitHub注册邮箱&quot; * 输入：ssh-keygen -t rsa -C &quot;GitHub注册邮箱&quot;，设置文件放的位置，保存生成的密匙（id_rsa，id_rsa.pub） * 然后进入GitHub账户，点击右上方头像旁边的向下三角箭头，选择Settings，进去后选择SSH and GPG keys，新建SSH key；自己给key起一个名字，然后将上一步生成的id_rsa.pub中的内容拷贝到key对话框，保存。 * 测试》仍然在刚才的GitBash中输入：ssh -T git@github.com，如果有提示输入（yes/no?）就输入y，然后回车； * 出现“Hi （GitHub用户名）....”等字样就说明配置成功了。 （2）配置deployment * 进入GitHub，点击第1步里面生成的那个仓库，找到右方中部“Clone or Doenload”，点击后会出现一个选择下拉框，把他变成“Clone with SSH”,然后复制下面地址栏的内容（git@github.com:dagongji10/dagongji10.github.io.git） * 找到Hexo》blog里面的配置文件_config.yml，打开，修改以下内容： deploy: type: git repo: git@github.com:dagongji10/dagongji10.github.io.git //这就是本地文件和GitHub建立联系的地方 branch: master //千万要注意冒号后面有一个空格，一定不能省略 * 保存之后回到Hexo》blog的GitBash，输入：hexo generate * 输入：hexo deploy（部署博客，需要等一会儿），看到Done：git的提示就好啦 （3）在浏览器地址栏输入https://（GitHub用户名）.github.io/就可以访问了 4、写博客与博客发布 * 进入本地博客目录，鼠标右键》Git Bash Here //进入hexo环境 * hexo clean //清除本地缓存 * hexo generate //生成个人博客所需的静态页面 * hexo new &quot;名称&quot; //新建博客 * 然后进入博客目录，找到 ..\source\_posts 路径 //这里就是博客的本地文件 * 用Markdown来修改博客 * hexo deploy //发布本地博客 5、布置自己的blog（参考） （1）在右上角或左上角实现fork me on GitHub： * 进入网址：https://github.com/blog/273-github-ribbons，选择喜欢的风格，复制代码； * 打开文件：themes/next/layout/_layout.swig，把刚才复制的代码放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面，并把href改为你的github地址 （2）添加RSS * 进入站点配置文件，找到Plugins，去掉前面的#，在后面添加hexo-generate-feed（plugins: hexo-generate-feed） * 进入主题配置文件，找到rss，在后面添加/atom.xml（rss: /atom.xml） （3）修改文章底部带“#”的标签 * 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成 &lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; （4）在文章末尾添加“本文结束”标记 （5）博文压缩 （6）在网站底部加上访客数量 （8）在线搜索：next官网》第三方服务有相关教程 （9）评论（来必力） * 来必力官网注册，点击网站右上角找到管理页面，在弹出的页面添加自己的网站地址，名字，之后复制代码里面data-uid后面的内容（带引号）； * 进入主题配置文件，修改livere_uid后面的值为上一步复制的代码，保存即可。 （10）在文章底部增加版权信息]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
